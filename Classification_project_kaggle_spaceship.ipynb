{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports**"
      ],
      "metadata": {
        "id": "wvFb_byOxF8g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1LU5DFOAZ5Y"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Functions for preprocessing**"
      ],
      "metadata": {
        "id": "Y-P7AUrcAmzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_split(df, columns=[], splitstr=\"None\", n=0, col_names=[]):\n",
        "    for column in columns:\n",
        "        split_cols = df[column].str.split(splitstr, expand=True)\n",
        "\n",
        "        for i in range(min(n, len(split_cols.columns))):\n",
        "            if i < len(col_names):\n",
        "                df[col_names[i]] = split_cols[i]\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "u87zKUDpAkTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_feature_sum(df, new_feature = [], col_names =[]):\n",
        "  for column in col_names:\n",
        "    df[new_feature] = df[column].sum()\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "hfgRr9kkAkQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_vals(df, cols_to_fill = [], fill_val = 0):\n",
        "  for col in cols_to_fill:\n",
        "    df[col].fillna(fill_val, inplace = True)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "bJW3VD0uAkOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_missing_with_lr(df):\n",
        "    df_filled = df.copy()\n",
        "    target_cols = df.columns[df.isnull().any()].tolist()\n",
        "    for col in target_cols:\n",
        "        missing_rows = df_filled[df_filled[col].isnull()]\n",
        "\n",
        "        for idx in missing_rows.index:\n",
        "\n",
        "            row_with_nan = df_filled.loc[idx].copy()\n",
        "            row_with_nan = row_with_nan.drop(labels=target_cols)\n",
        "            X_missing = row_with_nan.values.reshape(1, -1)\n",
        "\n",
        "\n",
        "            train_data = df_filled.dropna(subset=[col])\n",
        "            X_train = train_data.drop(columns=target_cols)\n",
        "            y_train = train_data[col]\n",
        "\n",
        "\n",
        "            model = LinearRegression()\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "            predicted_value = model.predict(X_missing)\n",
        "            df_filled.loc[idx, col] = predicted_value\n",
        "\n",
        "    return df_filled"
      ],
      "metadata": {
        "id": "WdAB6axdwbAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoder(df, columns=[], drop_first=False):\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    for column in columns:\n",
        "        dummies = pd.get_dummies(df[column], prefix=column, drop_first=drop_first, dtype=int)\n",
        "        df = pd.concat([df, dummies], axis=1)\n",
        "        df.drop(column, axis=1, inplace=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "CCdKtHDJcCgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bool_encoder(df, columns=[]):\n",
        "  for column in columns:\n",
        "    df[column].replace({True: 1, False: 0}, inplace=True)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "Erj11cxsZ_Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_cat(df, col, mapping):\n",
        "    df[col] = df[col].map(mapping)\n",
        "    return df"
      ],
      "metadata": {
        "id": "ZEx0MmzRmFS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cols_to_drop(df, cols_to_drop=[]):\n",
        "  for col in cols_to_drop:\n",
        "    df.drop(col, axis=1, inplace=True)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "St24yLny9HWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def discretize_data(df, cols_to_discretize, n_bins=4):\n",
        "  df = df.copy()\n",
        "  for column in cols_to_discretize:\n",
        "    discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal')\n",
        "    df[column] = discretizer.fit_transform(df[[column]])\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "IEfEdDoewYNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1 step dataframe preprocessing**"
      ],
      "metadata": {
        "id": "s0NR_bgpxMfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_preprocessing(\n",
        "    df,\n",
        "    split_cols_config=None,\n",
        "    add_feature_config=None,\n",
        "    fill_vals_config=None,\n",
        "    fill_missing_with_lr_enabled=False,\n",
        "    one_hot_encoder_config=None,\n",
        "    bool_encoder_config=None,\n",
        "    map_cat_config=None,\n",
        "    cols_to_drop_config=None\n",
        "):\n",
        "    def get_split(df, columns=[], splitstr=\"None\", n=0, col_names=[]):\n",
        "        for column in columns:\n",
        "            split_cols = df[column].str.split(splitstr, expand=True)\n",
        "            for i in range(min(n, len(split_cols.columns))):\n",
        "                if i < len(col_names):\n",
        "                    df[col_names[i]] = split_cols[i]\n",
        "        return df\n",
        "\n",
        "    def add_feature_sum(df, new_feature=None, col_names=[]):\n",
        "        if new_feature is not None:\n",
        "            df[new_feature] = df[col_names].sum(axis=1)\n",
        "        return df\n",
        "\n",
        "    def fill_vals(df, cols_to_fill=[], fill_val=0):\n",
        "        for col in cols_to_fill:\n",
        "            df[col].fillna(fill_val, inplace=True)\n",
        "        return df\n",
        "\n",
        "    def fill_missing_with_lr(df):\n",
        "        df_filled = df.copy()\n",
        "        target_cols = df.columns[df.isnull().any()].tolist()\n",
        "        for col in target_cols:\n",
        "            missing_rows = df_filled[df_filled[col].isnull()]\n",
        "            for idx in missing_rows.index:\n",
        "                row_with_nan = df_filled.loc[idx].copy()\n",
        "                row_with_nan = row_with_nan.drop(labels=target_cols)\n",
        "                X_missing = row_with_nan.values.reshape(1, -1)\n",
        "                train_data = df_filled.dropna(subset=[col])\n",
        "                X_train = train_data.drop(columns=target_cols)\n",
        "                y_train = train_data[col]\n",
        "                model = LinearRegression()\n",
        "                model.fit(X_train, y_train)\n",
        "                predicted_value = model.predict(X_missing)\n",
        "                df_filled.loc[idx, col] = predicted_value\n",
        "        return df_filled\n",
        "\n",
        "    def one_hot_encoder(df, columns=[], drop_first=False):\n",
        "        df = df.copy()\n",
        "        for column in columns:\n",
        "            dummies = pd.get_dummies(df[column], prefix=column, drop_first=drop_first, dtype=int)\n",
        "            df = pd.concat([df, dummies], axis=1)\n",
        "            df.drop(column, axis=1, inplace=True)\n",
        "        return df\n",
        "\n",
        "    def bool_encoder(df, columns=[]):\n",
        "        for column in columns:\n",
        "            df[column].replace({True: 1, False: 0}, inplace=True)\n",
        "        return df\n",
        "\n",
        "    def map_cat(df, col, mapping):\n",
        "        df[col] = df[col].map(mapping)\n",
        "        return df\n",
        "\n",
        "    def cols_to_drop(df, cols_to_drop=[]):\n",
        "        df.drop(cols_to_drop, axis=1, inplace=True)\n",
        "        return df\n",
        "\n",
        "    # Apply each preprocessing step based on the configurations passed\n",
        "    if split_cols_config:\n",
        "        df = get_split(df, **split_cols_config)\n",
        "\n",
        "    if add_feature_config:\n",
        "        df = add_feature_sum(df, **add_feature_config)\n",
        "\n",
        "    if fill_vals_config:\n",
        "        df = fill_vals(df, **fill_vals_config)\n",
        "\n",
        "    if fill_missing_with_lr_enabled:\n",
        "        df = fill_missing_with_lr(df)\n",
        "\n",
        "    if one_hot_encoder_config:\n",
        "        df = one_hot_encoder(df, **one_hot_encoder_config)\n",
        "\n",
        "    if bool_encoder_config:\n",
        "        df = bool_encoder(df, **bool_encoder_config)\n",
        "\n",
        "    if map_cat_config:\n",
        "        for config in map_cat_config:\n",
        "            df = map_cat(df, **config)\n",
        "\n",
        "    if cols_to_drop_config:\n",
        "        df = cols_to_drop(df, **cols_to_drop_config)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "mXYNDpfxxYzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Models training**"
      ],
      "metadata": {
        "id": "DVkxV1Q0xZof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(\"Transported\", axis =1)\n",
        "y = df[\"Transported\"]"
      ],
      "metadata": {
        "id": "RzMOdsO3xeYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For models using discretized data\n",
        "\n",
        "X_d = dsc_fin.drop(\"Transported\", axis =1)\n",
        "y_d = dsc_fin[\"Transported\"]"
      ],
      "metadata": {
        "id": "CrBFWXNU2hy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = LogisticRegression(solver = \"liblinear\", penalty = \"l1\", max_iter = 200)\n",
        "model_1.fit(X, y)\n",
        "y_pred_m1= model_1.predict(X)"
      ],
      "metadata": {
        "id": "BRYlYdoX2hwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = LogisticRegression(solver = \"liblinear\", penalty = \"l1\", max_iter = 200)\n",
        "model_2.fit(X_d, y_d)\n",
        "y_pred_m2= model_2.predict(X_d)\n"
      ],
      "metadata": {
        "id": "5vgGajzx2hto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_estimators': 200,\n",
        "    'max_features': 'sqrt',\n",
        "    'max_depth': None,\n",
        "    'min_samples_split': 2,\n",
        "    'min_samples_leaf': 2,\n",
        "    'bootstrap': True,\n",
        "    'criterion': 'entropy'\n",
        "}\n",
        "\n",
        "model_3 = RandomForestClassifier(**param_grid, random_state=42)\n",
        "model_3.fit(X, y)\n",
        "\n",
        "y_pred_m3 = model_3.predict(X)"
      ],
      "metadata": {
        "id": "eC-qh4JW3DUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=2, min_samples_leaf=2,\n",
        "                            max_features='sqrt', bootstrap=True, criterion='entropy', random_state=42)\n",
        "rf.fit(X, y)\n",
        "model_4 = AdaBoostClassifier(base_estimator=rf, n_estimators=50, random_state=42)\n",
        "model_4.fit(X, y)\n",
        "y_pred_ada_nd= model_4.predict(X)"
      ],
      "metadata": {
        "id": "4DKC4KEG3DRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=2, min_samples_leaf=2,\n",
        "                            max_features='sqrt', bootstrap=True, criterion='entropy', random_state=42)\n",
        "rf.fit(X_d, y_d)\n",
        "\n",
        "model_5 = AdaBoostClassifier(base_estimator=rf, n_estimators=50, random_state=42)\n",
        "model_5.fit(X_d, y_d)\n",
        "y_pred_ada_d = model_5.predict(X_d)"
      ],
      "metadata": {
        "id": "bgxZ7T_b3DOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_6 = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=2, min_samples_leaf=2,\n",
        "                            max_features='sqrt', bootstrap=True, criterion='entropy', random_state=42)\n",
        "model_6.fit(X_d, y_d)\n",
        "y_pred_rf_d= model_6.predict(X_d)"
      ],
      "metadata": {
        "id": "QeGetoWm3DL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_7 = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=2, min_samples_leaf=2,\n",
        "                            max_features='sqrt', bootstrap=True, criterion='entropy', random_state=42)\n",
        "model_7.fit(X, y)\n",
        "y_pred_rf_d= model_7.predict(X)"
      ],
      "metadata": {
        "id": "enKCHRjj2hrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=200, max_depth=9, min_samples_split=2, min_samples_leaf=2,\n",
        "                            max_features='sqrt', bootstrap=True, criterion='entropy', random_state=42)\n",
        "rf.fit(X_d, y_d)\n",
        "\n",
        "model_8 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model_8.fit(X_d, y_d)\n",
        "y_pred_gb_d= model_8.predict(X_d)"
      ],
      "metadata": {
        "id": "sSz-sZsN2hok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=200, max_depth=9, min_samples_split=2, min_samples_leaf=2,\n",
        "                            max_features='sqrt', bootstrap=True, criterion='entropy', random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "model_9 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model_9.fit(X, y)\n",
        "y_pred_gb_d= model_9.predict(X)"
      ],
      "metadata": {
        "id": "S6guck3X3aMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ensemble model with Voting classifier**"
      ],
      "metadata": {
        "id": "ECgziaVh3fAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_model = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('log_reg', model_1),\n",
        "        ('log_reg_dsc', model_2),\n",
        "        ('rf', model_3),\n",
        "        ('ada_boost_nd', model_4),\n",
        "        ('ada_boost_d', model_5),\n",
        "        ('rf_dsc', model_6),\n",
        "        ('rf_nd', model_7),\n",
        "        ('gb_dsc', model_8),\n",
        "        ('gb_nd', model_9)\n",
        "    ],\n",
        "    voting='hard' )"
      ],
      "metadata": {
        "id": "riotbTvc3aJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_model.fit(X, y)\n",
        "y_pred = ensemble_model.predict(X)\n",
        "accuracy_score(y, y_pred)"
      ],
      "metadata": {
        "id": "xyC16oAq3aHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_pred = ensemble_model.predict(df_kaggle)\n",
        "submission = submission_pred.astype(bool)"
      ],
      "metadata": {
        "id": "TiQUn2dd3aES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "passenger_ids = new_submission['PassengerId'].values\n",
        "transported_predictions = submission\n",
        "\n",
        "sub_df = pd.DataFrame({\n",
        "    \"PassengerId\": passenger_ids,\n",
        "    \"Transported\": transported_predictions\n",
        "})\n",
        "\n",
        "print(sub_df.head())"
      ],
      "metadata": {
        "id": "mgPp3Pi54EwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df.to_csv(\"ensemble_model_submission.csv\", index = False)"
      ],
      "metadata": {
        "id": "BszWC06_4Etq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}